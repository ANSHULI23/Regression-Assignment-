{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Regression**"
      ],
      "metadata": {
        "id": "Kd-IafFrBJuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "###**1. What is Simple Linear Regression?**  \n",
        "\n",
        "- Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X) using a straight-line equation:\n",
        "\n",
        "\\[\n",
        "Y = mX + c\n",
        "\\]\n",
        "\n",
        "where \\( m \\) is the slope and \\( c \\) is the intercept.\n",
        "\n",
        "------\n",
        "\n",
        "###**2. What are the key assumptions of Simple Linear Regression?**  \n",
        "- **Linearity**: The relationship between X and Y is linear.  \n",
        "- **Independence**: The observations are independent.  \n",
        "- **Homoscedasticity**: Constant variance of residuals.  \n",
        "- **Normality**: The residuals follow a normal distribution.  \n",
        "- **No multicollinearity**: Since there's only one independent variable, this assumption is not relevant.  \n",
        "\n",
        "-----\n",
        "\n",
        "###**3. What does the coefficient \\( m \\) represent in the equation \\( Y = mX + c \\)?**  \n",
        "\n",
        "- The coefficient \\( m \\) represents the slope of the regression line, indicating the rate of change in \\( Y \\) for every one-unit increase in \\( X \\).  \n",
        "\n",
        "-----\n",
        "\n",
        "###**4. What does the intercept \\( c \\) represent in the equation \\( Y = mX + c \\)?**  \n",
        "\n",
        "- The intercept \\( c \\) represents the value of \\( Y \\) when \\( X \\) is 0. It is the point where the regression line crosses the Y-axis.\n",
        "  \n",
        "-----"
      ],
      "metadata": {
        "id": "KEqkCct3BNbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### **5. How do we calculate the slope \\( m \\) in Simple Linear Regression?**\n",
        "\n",
        "-\n",
        "The slope \\( m \\) is calculated using the formula:  \n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- \\( $ X_i$, $Y_i $\\) are individual data points  \n",
        "- \\( $\\bar{X}$, $\\bar{Y}$ \\) are the mean values of \\( X \\) and \\( Y \\), respectively  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Yyw_YYHxLJ-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_slope(x, y):\n",
        "    \"\"\"\n",
        "    Calculate the slope (m) in Simple Linear Regression.\n",
        "\n",
        "    Parameters:\n",
        "    x (array-like): Independent variable data.\n",
        "    y (array-like): Dependent variable data.\n",
        "\n",
        "    Returns:\n",
        "    float: Slope of the regression line.\n",
        "    \"\"\"\n",
        "    x_mean = np.mean(x)\n",
        "    y_mean = np.mean(y)\n",
        "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
        "    denominator = np.sum((x - x_mean) ** 2)\n",
        "    m = numerator / denominator\n",
        "    return m\n",
        "\n",
        "# Example usage:\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 3, 5, 7, 11])\n",
        "slope = calculate_slope(x, y)\n",
        "print(f\"The calculated slope is: {slope}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FgNH9TvKPVc",
        "outputId": "5bb65044-0638-480f-8062-673f7cda441a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The calculated slope is: 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "###**6. What is the purpose of the least squares method in Simple Linear Regression?**  \n",
        "-\n",
        "The least squares method minimizes the sum of squared residuals (differences between actual and predicted values) to find the best-fitting regression line.  \n",
        "\n",
        "-----\n",
        "\n",
        "###**7. How is the coefficient of determination (\\( R^2 \\)) interpreted in Simple Linear Regression?**\n",
        "-  \n",
        "\\( R^2 \\) measures the proportion of variance in the dependent variable explained by the independent variable. It ranges from 0 to 1, where a higher value indicates a better fit.  \n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "wd-CvmIRNhzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Actual values\n",
        "y_true = [3, -0.5, 2, 7]\n",
        "\n",
        "# Predicted values\n",
        "y_pred = [2.5, 0.0, 2, 8]\n",
        "\n",
        "# Calculate R^2 score\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "print(f\"R^2 score: {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brk8k0kcNxVF",
        "outputId": "3c56ffbc-50e0-4d25-929e-4d91ffcf9785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: 0.9486081370449679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. What is Multiple Linear Regression?**\n",
        "\n",
        "-\n",
        "Multiple Linear Regression models the relationship between a dependent variable and multiple independent variables using the equation:  \n",
        "\n",
        "$$\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- \\( Y \\) = Dependent variable  \n",
        "- \\($ X_1, X_2, ..., X_n $\\) = Independent variables  \n",
        "- \\( $b_0$ \\) = Intercept  \n",
        "- \\($ b_1, b_2, ..., b_n$ \\) = Regression coefficients.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "LCaT8fHeNqVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**9. What is the main difference between Simple and Multiple Linear Regression?**  \n",
        "The main difference is the number of independent variables:  \n",
        "- Simple Linear Regression has only one independent variable.  \n",
        "- Multiple Linear Regression has two or more independent variables.  \n",
        "\n",
        "-----\n",
        "\n",
        "###**10. What are the key assumptions of Multiple Linear Regression?**  \n",
        "\n",
        "- **Linearity**: The relationship between dependent and independent variables is linear.  \n",
        "- **Independence**: The observations are independent.  \n",
        "- **Homoscedasticity**: Constant variance of residuals.  \n",
        "- **Normality**: Residuals follow a normal distribution.  \n",
        "- **No multicollinearity**: Independent variables should not be highly correlated.  \n",
        "\n",
        "-----\n",
        "###**11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "\n",
        "- **Heteroscedasticity** occurs when the variance of residuals is not constant across all levels of an independent variable. This can lead to:  \n",
        "- Biased standard errors, affecting hypothesis tests.  \n",
        "- Reduced reliability of confidence intervals.  \n",
        "- Misleading significance of predictors.  \n",
        "\n",
        "---\n",
        "\n",
        "###**12. How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "-\n",
        "To reduce **multicollinearity**, you can:  \n",
        "- **Remove highly correlated variables** (feature selection).  \n",
        "- **Use Principal Component Analysis (PCA)** to transform features.  \n",
        "- **Apply Ridge or Lasso Regression**, which regularizes coefficients.  \n",
        "- **Increase the sample size** to stabilize coefficient estimates.  \n",
        "\n",
        "---\n",
        "\n",
        "###**13. What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "-  \n",
        "Common methods include:  \n",
        "- **One-Hot Encoding**: Converts categories into binary columns.  \n",
        "- **Label Encoding**: Assigns numerical labels to categories.  \n",
        "- **Ordinal Encoding**: Used for ordinal data (e.g., \"low\", \"medium\", \"high\").  \n",
        "- **Dummy Variables**: Creates binary variables, avoiding dummy variable trap by excluding one category.  \n",
        "\n",
        "---\n",
        "\n",
        "###**14. What is the role of interaction terms in Multiple Linear Regression?**\n",
        "  \n",
        "**Interaction terms** model the combined effect of two variables on the dependent variable. The regression equation is:  \n",
        "\n",
        "$$\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + b_3(X_1 \\times X_2)\n",
        "$$  \n",
        "\n",
        "where \\( b_3 \\) represents the interaction effect between \\( X_1 \\) and \\( X_2 \\).  \n",
        "\n",
        "---\n",
        "\n",
        "###**15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "- In **Simple Linear Regression**, the intercept is the expected value of \\( Y \\) when \\( X = 0 \\).  \n",
        "- In **Multiple Linear Regression**, the intercept represents the expected \\( Y \\) when all independent variables are zero, which may not always have a meaningful interpretation.  \n",
        "\n",
        "---\n",
        "\n",
        "###**16. What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "\n",
        "-\n",
        "The **slope coefficient** indicates the expected change in \\( Y \\) for a one-unit increase in \\( X \\), assuming all other variables remain constant.  \n",
        "- **Positive slope** → Increase in \\( X \\) leads to an increase in \\( Y \\).  \n",
        "- **Negative slope** → Increase in \\( X \\) leads to a decrease in \\( Y \\).  \n",
        "\n",
        "---\n",
        "\n",
        "###**17. How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "-\n",
        "The **intercept** represents the predicted value of \\( Y \\) when all independent variables are zero. However, it is meaningful only if:  \n",
        "- A zero value for independent variables makes sense.  \n",
        "- The range of data includes zero.  \n",
        "\n",
        "---\n",
        "\n",
        "###**18. What are the limitations of using \\( R^2 \\) as a sole measure of model performance?**\n",
        "\n",
        "- **Does not indicate causality** between variables.  \n",
        "- **Sensitive to added predictors** (even if they are irrelevant).  \n",
        "- **Cannot detect overfitting** (high \\( R^2 \\) does not mean a good model).  \n",
        "- **Adjusted \\( R^2 \\)** is preferred as it accounts for the number of predictors.  \n",
        "\n",
        "---\n",
        "\n",
        "###**19. How would you interpret a large standard error for a regression coefficient?**\n",
        "-\n",
        "A **large standard error** indicates:  \n",
        "- High variability in the estimated coefficient.  \n",
        "- Weak statistical significance of the predictor.  \n",
        "- Possible multicollinearity or small sample size.  \n",
        "\n",
        "---\n",
        "\n",
        "###**20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "\n",
        "**Detection methods**:  \n",
        "- **Residual vs. Fitted plot**: If residuals show a funnel shape, heteroscedasticity is present.  \n",
        "- **Breusch-Pagan test**: A statistical test for heteroscedasticity.  \n",
        "\n",
        "**Why it matters**:  \n",
        "- Affects confidence intervals and hypothesis testing.  \n",
        "- Leads to inefficient estimates and incorrect conclusions.  \n",
        "- Solutions include log transformations, weighted least squares, or robust standard errors.\n",
        "\n",
        "----\n",
        "\n",
        "###**21. What does it mean if a Multiple Linear Regression model has a high \\( R^2 \\) but low adjusted \\( R^2 \\)?**\n",
        "-   \n",
        "A high \\( R^2 \\) but low **adjusted \\( R^2 \\)** suggests that:  \n",
        "- The model includes irrelevant variables that do not contribute much to prediction.  \n",
        "- The model might be **overfitting** by adding unnecessary predictors.  \n",
        "- **Adjusted \\( R^2 \\)** penalizes excessive predictors, making it a better metric for model evaluation.  \n",
        "\n",
        "---\n",
        "\n",
        "###**22. Why is it important to scale variables in Multiple Linear Regression?**\n",
        "\n",
        "- Scaling variables ensures that:  \n",
        "- **Gradient Descent converges faster** (important for optimization).  \n",
        "- **Coefficients are comparable**, preventing large-scale differences.  \n",
        "- **Regularization methods (Ridge/Lasso) work correctly**, as unscaled data can bias penalty terms.  \n",
        "- **Prevents numerical instability** in matrix computations.  \n",
        "\n",
        "Common scaling techniques:  \n",
        "- **Standardization**: \\( X' = \\frac{X - \\mu}{\\sigma} \\) (mean = 0, std = 1).  \n",
        "- **Normalization**: \\( X' = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}} \\) (scales to [0,1]).  \n",
        "\n",
        "---\n",
        "\n",
        "###**23. What is polynomial regression?**\n",
        "\n",
        "**Polynomial regression** extends linear regression by introducing higher-degree terms:  \n",
        "\n",
        "$$\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\dots + b_nX^n\n",
        "$$\n",
        "\n",
        "This allows modeling of **non-linear** relationships using a polynomial function.  \n",
        "\n",
        "---\n",
        "\n",
        "###**24. How does polynomial regression differ from linear regression?**\n",
        "  \n",
        "- **Linear Regression** assumes a straight-line relationship between \\( X \\) and \\( Y \\).  \n",
        "- **Polynomial Regression** captures **curved** relationships by adding power terms.  \n",
        "- Despite being a non-linear relationship, **it is still a linear model in terms of coefficients**.  \n",
        "\n",
        "---\n",
        "\n",
        "###**25. When is polynomial regression used?**\n",
        "-   \n",
        "Polynomial regression is useful when:\n",
        "- The data shows **curvature**, meaning a straight line does not fit well.  \n",
        "- **Higher-order relationships** exist between dependent and independent variables.  \n",
        "- **Quadratic** (degree 2) or **cubic** (degree 3) trends appear in residual plots.  \n",
        "\n",
        "---\n",
        "\n",
        "###**26. What is the general equation for polynomial regression?**\n",
        "\n",
        "- The general equation is:  \n",
        "\n",
        "$$\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\dots + b_nX^n\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- \\( b_0 \\) is the intercept.  \n",
        "- \\( b_1, b_2, ... b_n \\) are the polynomial coefficients.  \n",
        "- \\( X^n \\) represents higher-order terms for better curve fitting.  \n",
        "\n",
        "---\n",
        "\n",
        "###**27. Can polynomial regression be applied to multiple variables?**\n",
        "\n",
        "- Yes, **Polynomial Regression** can be extended to **multiple variables**, resulting in **Polynomial Multiple Regression**:  \n",
        "\n",
        "$$\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + b_3X_1^2 + b_4X_2^2 + b_5X_1X_2 + \\dots\n",
        "$$\n",
        "\n",
        "This allows capturing **complex interactions** between variables.  \n",
        "\n",
        "---\n",
        "\n",
        "###**28. What are the limitations of polynomial regression?**\n",
        "\n",
        "- **Overfitting**: High-degree polynomials can fit noise instead of patterns.  \n",
        "- **Extrapolation issues**: Predictions outside the data range become unreliable.  \n",
        "- **Computational cost**: High-degree terms increase complexity and instability.  \n",
        "- **Collinearity**: Higher-order terms are often correlated, leading to multicollinearity.  \n",
        "\n",
        "---\n",
        "\n",
        "###**29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "- **Adjusted \\( R^2 \\)**: Helps compare models by penalizing excessive predictors.  \n",
        "- **Cross-validation**: Prevents overfitting by testing on unseen data.  \n",
        "- **Akaike Information Criterion (AIC) / Bayesian Information Criterion (BIC)**: Penalizes model complexity.  \n",
        "- **Residual plots**: Helps visualize errors and non-linearity.  \n",
        "\n",
        "---\n",
        "\n",
        "###**30. Why is visualization important in polynomial regression?**\n",
        "\n",
        "- **Detects non-linearity**: Scatter plots reveal trends in data.  \n",
        "- **Identifies overfitting**: A highly fluctuating curve suggests excessive polynomial degrees.  \n",
        "- **Evaluates model performance**: Residual plots help in diagnosing errors.  \n",
        "- **Communicates findings**: Easy to explain complex relationships visually.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VB3WpGsdPcAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. How is polynomial regression implemented in Python?\n",
        "def implement_polynomial_regression():\n",
        "    \"\"\"\n",
        "    Example using sklearn:\n",
        "    ```python\n",
        "    from sklearn.preprocessing import PolynomialFeatures\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    import numpy as np\n",
        "\n",
        "    X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "    y = np.array([2, 5, 10, 17, 26])\n",
        "\n",
        "    poly = PolynomialFeatures(degree=2)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "    y_pred = model.predict(X_poly)\n",
        "    ```\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "rbV4qRu6Pzb5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}